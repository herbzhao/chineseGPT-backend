{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import (\n",
    "    ConversationChain,\n",
    "    LLMChain,\n",
    "    RetrievalQA,\n",
    "    SequentialChain,\n",
    "    ConversationalRetrievalChain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "from langchain.document_loaders import BSHTMLLoader, PyPDFLoader, TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.memory import (\n",
    "    ChatMessageHistory,\n",
    "    CombinedMemory,\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    SimpleMemory,\n",
    ")\n",
    "\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.prompts.chat import (\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, LLMResult, SystemMessage\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "from langchain.vectorstores import FAISS, Chroma, Pinecone\n",
    "from pydantic import BaseModel, validator\n",
    "import pinecone\n",
    "\n",
    "from parameters import (\n",
    "    ACCURACY_TEMPERATURE_MAP,\n",
    "    HISTORY_MAX_LENGTH,\n",
    "    HISTORY_MAX_TEXT,\n",
    "    MODEL,\n",
    "    system_prompts,\n",
    ")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(\".env.local\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(\n",
    "    accuracy: str = \"medium\",\n",
    "    stream: bool = True,\n",
    "    model: str = \"gpt-3.5-turbo\",  # Replace with your model\n",
    "    session_id: Optional[str] = None,\n",
    ") -> str:\n",
    "    chat = ChatOpenAI(\n",
    "        model_name=model,\n",
    "        temperature=ACCURACY_TEMPERATURE_MAP[accuracy],\n",
    "        streaming=stream,\n",
    "        # callbacks=[MyCustomAsyncHandler()],\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    )\n",
    "    return chat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_python_code_base_to_split_texts(folder_path: Path, filetype: str = \".py\"):\n",
    "\n",
    "    if not folder_path.is_dir():\n",
    "        raise ValueError(\"folder_path must be a directory\")\n",
    "    documents = []\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for file in filenames:\n",
    "            if file.endswith(filetype) and \"/.venv/\" not in dirpath:\n",
    "                try:\n",
    "                    loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                    documents.extend(loader.load_and_split())\n",
    "                    # documents.extend(loader.load())\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "    print(len(documents))\n",
    "    python_splitter = PythonCodeTextSplitter(chunk_size=500, chunk_overlap=0)     \n",
    "    texts = python_splitter.split_documents(documents)\n",
    "    print(len(texts))\n",
    "    return texts\n",
    "\n",
    "def create_pinecone_index(texts, embeddings, index_name):\n",
    "    pinecone.init(\n",
    "        api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "        environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console\n",
    "    )\n",
    "    active_indexes = pinecone.list_indexes()\n",
    "    if index_name not in active_indexes:\n",
    "        pinecone.create_index(\n",
    "            index_name,\n",
    "            dimension=1536,\n",
    "            metric=\"cosine\",\n",
    "            pods=1,\n",
    "            replicas=1,\n",
    "            pod_type=\"Starter\",\n",
    "        )\n",
    "    db = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
    "    return db\n",
    "\n",
    "def create_chroma_collection(texts, embedding_function, collection_name=\"example\", persist_directory=\"resources\\chroma\"):\n",
    "    # check if the directory exists\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386\n",
      "9336\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"wikipedia\"\n",
    "chromadb_persist_directory = 'resources\\chromadb'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "local_embedding_function = SentenceTransformerEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "# adding document to the db\n",
    "loader = TextLoader(\"resources\\example.txt\", encoding=\"utf8\")\n",
    "# loader = PyPDFLoader(\"resources\\gpt4_explain.pdf\")\n",
    "# loader = BSHTMLLoader(\"resources\\gpt4_explain.html\", open_encoding=\"utf8\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# adding codebase to the db\n",
    "texts = convert_python_code_base_to_split_texts(\n",
    "    Path(\"resources\\langchain\"), filetype=\".py\"\n",
    ")\n",
    "\n",
    "collection_name = \"langchain_codebase\"\n",
    "db = create_chroma_collection(texts, embedding_function=local_embedding_function, collection_name=collection_name, persist_directory=chromadb_persist_directory)\n",
    "\n",
    "# db = Chroma.from_documents(\n",
    "#     documents=texts,\n",
    "#     embedding=local_embedding_function,\n",
    "#     persist_directory=\"resources\\chroma\",\n",
    "#     collection_name=\"fffff\",\n",
    "# )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='langchain_codebase' id=UUID('4eab5dc8-8b2a-422b-9e91-fb24a92bb1b4') metadata=None\n",
      "name='langchain_codebase' id=UUID('4eab5dc8-8b2a-422b-9e91-fb24a92bb1b4') metadata=None\n"
     ]
    }
   ],
   "source": [
    "print(db._collection)\n",
    "\n",
    "db.persist()\n",
    "db = None\n",
    "db = Chroma(persist_directory=chromadb_persist_directory, embedding_function=local_embedding_function, collection_name=collection_name)\n",
    "\n",
    "print(db._collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='langchain_codebase' id=UUID('4eab5dc8-8b2a-422b-9e91-fb24a92bb1b4') metadata=None\n",
      "[Document(page_content='test_chat_openai_streaming() -> None:\\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\\n    callback_handler = FakeCallbackHandler()\\n    callback_manager = CallbackManager([callback_handler])\\n    chat = ChatOpenAI(\\n        max_tokens=10,\\n        streaming=True,\\n        temperature=0,\\n        callback_manager=callback_manager,\\n        verbose=True,\\n    )\\n    message = HumanMessage(content=\"Hello\")\\n    response = chat([message])\\n    assert callback_handler.llm_streams > 0', metadata={'source': 'resources\\\\langchain\\\\tests\\\\integration_tests\\\\chat_models\\\\test_openai.py'}), Document(page_content='test_promptlayer_chat_openai_streaming() -> None:\\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\\n    callback_handler = FakeCallbackHandler()\\n    callback_manager = CallbackManager([callback_handler])\\n    chat = PromptLayerChatOpenAI(\\n        max_tokens=10,\\n        streaming=True,\\n        temperature=0,\\n        callback_manager=callback_manager,\\n        verbose=True,\\n    )\\n    message = HumanMessage(content=\"Hello\")\\n    response = chat([message])', metadata={'source': 'resources\\\\langchain\\\\tests\\\\integration_tests\\\\chat_models\\\\test_promptlayer_openai.py'})]\n"
     ]
    }
   ],
   "source": [
    "print(db._collection)\n",
    "print(db.similarity_search(\"How to stop streaming in ChatOpenAI?\", k=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = create_chat(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# to load a vector store\n",
    "loaded_db = Chroma(persist_directory=chromadb_persist_directory, embedding_function=local_embedding_function, collection_name=collection_name)\n",
    "\n",
    "# loaded_db.persist()\n",
    "# print(loaded_db.similarity_search(\"who discovered the mode?\", k=2))\n",
    "\n",
    "# actually use it in a chain as data source\n",
    "# https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "# chat over documents\n",
    "memory = ConversationBufferMemory(output_key='answer', memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat,\n",
    "    retriever=loaded_db.as_retriever(k=3),\n",
    "    return_source_documents=True,\n",
    "    memory = memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "_create_retry_decorator(llm: ChatOpenAI) -> Callable[[Any], Any]:\n",
      "    import openai\n",
      "\n",
      "test_openai_chat_streaming_callback() -> None:\n",
      "    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n",
      "    callback_handler = FakeCallbackHandler()\n",
      "    callback_manager = CallbackManager([callback_handler])\n",
      "    llm = OpenAIChat(\n",
      "        max_tokens=10,\n",
      "        streaming=True,\n",
      "        temperature=0,\n",
      "        callback_manager=callback_manager,\n",
      "        verbose=True,\n",
      "    )\n",
      "    llm(\"Write me a sentence with 100 words.\")\n",
      "    assert callback_handler.llm_streams != 0\n",
      "\n",
      "test_chat_openai_streaming() -> None:\n",
      "    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n",
      "    callback_handler = FakeCallbackHandler()\n",
      "    callback_manager = CallbackManager([callback_handler])\n",
      "    chat = ChatOpenAI(\n",
      "        max_tokens=10,\n",
      "        streaming=True,\n",
      "        temperature=0,\n",
      "        callback_manager=callback_manager,\n",
      "        verbose=True,\n",
      "    )\n",
      "    message = HumanMessage(content=\"Hello\")\n",
      "    response = chat([message])\n",
      "    assert callback_handler.llm_streams > 0\n",
      "\n",
      "params = self.dict()\n",
      "        params[\"stop\"] = stop\n",
      "\n",
      "        callback_manager = CallbackManager.configure(\n",
      "            callbacks, self.callbacks, self.verbose\n",
      "        )\n",
      "        run_manager = callback_manager.on_chat_model_start(\n",
      "            {\"name\": self.__class__.__name__}, messages, invocation_params=params\n",
      "        )\n",
      "Human: How to stop callback in ChatOpenAI?\u001b[0m\n",
      "Based on the given context, it seems that you can stop callbacks in ChatOpenAI by passing a \"stop\" parameter to the \"params\" dictionary. However, the exact implementation may depend on the specific use case and the structure of the codebase.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = chain(\n",
    "    {\n",
    "        \"question\": \"How to stop callback in ChatOpenAI?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To stop streaming in ChatOpenAI, you can simply set the `streaming` parameter to `False` when you create the `ChatOpenAI` object. For example:\n",
      "\n",
      "```\n",
      "chat = ChatOpenAI(\n",
      "    max_tokens=10,\n",
      "    streaming=False,  # set streaming to False to stop it\n",
      "    temperature=0,\n",
      "    callback_manager=callback_manager,\n",
      "    verbose=True,\n",
      ")\n",
      "```\n",
      "\n",
      "This will stop the streaming behavior and return the full response at once instead of returning partial responses as they are generated.\n",
      "page_content='test_chat_openai_streaming() -> None:\\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\\n    callback_handler = FakeCallbackHandler()\\n    callback_manager = CallbackManager([callback_handler])\\n    chat = ChatOpenAI(\\n        max_tokens=10,\\n        streaming=True,\\n        temperature=0,\\n        callback_manager=callback_manager,\\n        verbose=True,\\n    )\\n    message = HumanMessage(content=\"Hello\")\\n    response = chat([message])\\n    assert callback_handler.llm_streams > 0' metadata={'source': 'resources\\\\langchain\\\\tests\\\\integration_tests\\\\chat_models\\\\test_openai.py'}\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])\n",
    "\n",
    "print(response[\"source_documents\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How to stop callback in ChatOpenAI?\n",
      "Assistant: Based on the given context, it seems that you can stop callbacks in ChatOpenAI by passing a \"stop\" parameter to the \"params\" dictionary. However, the exact implementation may depend on the specific use case and the structure of the codebase.\n",
      "Follow Up Input: Can you write a short example?\n",
      "Standalone question:\u001b[0m\n",
      "Could you provide a brief example of how to stop callbacks in ChatOpenAI by passing a \"stop\" parameter to the \"params\" dictionary?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "params = self.dict()\n",
      "        params[\"stop\"] = stop\n",
      "\n",
      "        callback_manager = CallbackManager.configure(\n",
      "            callbacks, self.callbacks, self.verbose\n",
      "        )\n",
      "        run_manager = callback_manager.on_chat_model_start(\n",
      "            {\"name\": self.__class__.__name__}, messages, invocation_params=params\n",
      "        )\n",
      "\n",
      "def _call(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        stop: Optional[List[str]] = None,\n",
      "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
      "    ) -> str:\n",
      "        \"\"\"Call the GooseAI API.\"\"\"\n",
      "        params = self._default_params\n",
      "        if stop is not None:\n",
      "            if \"stop\" in params:\n",
      "                raise ValueError(\"`stop` found in both the input and default params.\")\n",
      "            params[\"stop\"] = stop\n",
      "\n",
      "def _get_chat_params(\n",
      "        self, prompts: List[str], stop: Optional[List[str]] = None\n",
      "    ) -> Tuple:\n",
      "        if len(prompts) > 1:\n",
      "            raise ValueError(\n",
      "                f\"OpenAIChat currently only supports single prompt, got {prompts}\"\n",
      "            )\n",
      "        messages = self.prefix_messages + [{\"role\": \"user\", \"content\": prompts[0]}]\n",
      "        params: Dict[str, Any] = {**{\"model\": self.model_name}, **self._default_params}\n",
      "        if stop is not None:\n",
      "            if \"stop\" in params:\n",
      "\n",
      "raise ValueError(\"`stop` found in both the input and default params.\")\n",
      "            params[\"stop\"] = stop\n",
      "        if params.get(\"max_tokens\") == -1:\n",
      "            # for ChatGPT api, omitting max_tokens is equivalent to having no limit\n",
      "            del params[\"max_tokens\"]\n",
      "        return messages, params\n",
      "Human: Could you provide a brief example of how to stop callbacks in ChatOpenAI by passing a \"stop\" parameter to the \"params\" dictionary?\u001b[0m\n",
      "Yes, to stop callbacks in ChatOpenAI, you can pass a \"stop\" parameter to the \"params\" dictionary. Here's an example:\n",
      "\n",
      "```\n",
      "from chat_openai import OpenAIChat\n",
      "\n",
      "# initialize the OpenAIChat object\n",
      "chat = OpenAIChat(model_name=\"davinci\")\n",
      "\n",
      "# define the prompts and stop criteria\n",
      "prompts = [\"Hello, how are you today?\"]\n",
      "stop = [\"Thanks for chatting, bye!\"]\n",
      "\n",
      "# get the chat parameters\n",
      "messages, params = chat._get_chat_params(prompts=prompts, stop=stop)\n",
      "\n",
      "# call the API and pass the stop parameter to the params dictionary\n",
      "response = chat._call(prompt=prompts[0], run_manager=None, stop=stop, params=params)\n",
      "\n",
      "# print the API response\n",
      "print(response)\n",
      "```\n",
      "\n",
      "In this example, we first initialize an OpenAIChat object and define the prompts and stop criteria. We then get the chat parameters using the `_get_chat_params` method, passing the prompts and stop criteria as arguments. Finally, we call the API using the `_call` method, passing the prompt, run_manager, stop criteria, and params dictionary as arguments. The `stop` parameter is included in the `params` dictionary to stop the callbacks.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = chain(\n",
    "    {\n",
    "        \"question\": \"Can you write a short example?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, to stop callbacks in ChatOpenAI, you can pass a \"stop\" parameter to the \"params\" dictionary. Here's an example:\n",
      "\n",
      "```\n",
      "from chat_openai import OpenAIChat\n",
      "\n",
      "# initialize the OpenAIChat object\n",
      "chat = OpenAIChat(model_name=\"davinci\")\n",
      "\n",
      "# define the prompts and stop criteria\n",
      "prompts = [\"Hello, how are you today?\"]\n",
      "stop = [\"Thanks for chatting, bye!\"]\n",
      "\n",
      "# get the chat parameters\n",
      "messages, params = chat._get_chat_params(prompts=prompts, stop=stop)\n",
      "\n",
      "# call the API and pass the stop parameter to the params dictionary\n",
      "response = chat._call(prompt=prompts[0], run_manager=None, stop=stop, params=params)\n",
      "\n",
      "# print the API response\n",
      "print(response)\n",
      "```\n",
      "\n",
      "In this example, we first initialize an OpenAIChat object and define the prompts and stop criteria. We then get the chat parameters using the `_get_chat_params` method, passing the prompts and stop criteria as arguments. Finally, we call the API using the `_call` method, passing the prompt, run_manager, stop criteria, and params dictionary as arguments. The `stop` parameter is included in the `params` dictionary to stop the callbacks.\n",
      "page_content='params = self.dict()\\n        params[\"stop\"] = stop\\n\\n        callback_manager = CallbackManager.configure(\\n            callbacks, self.callbacks, self.verbose\\n        )\\n        run_manager = callback_manager.on_chat_model_start(\\n            {\"name\": self.__class__.__name__}, messages, invocation_params=params\\n        )' metadata={'source': 'resources\\\\langchain\\\\langchain\\\\chat_models\\\\base.py'}\n",
      "dict_keys(['question', 'chat_history', 'answer', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])\n",
    "\n",
    "print(response[\"source_documents\"][0])\n",
    "\n",
    "print(response.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('chineseGPT-backend-X6dBi7bl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f7d86aacf80c9bc25892b63dd26ecb934113661019d3f27cd47c547acb76f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
